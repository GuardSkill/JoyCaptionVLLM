JoyCaption VLLM API å·¥å…·é›†
English | ä¸­æ–‡

<a id="chinese">ğŸ¨ JoyCaption VLLM API å·¥å…·é›†</a>
åŸºäº VLLM çš„ JoyCaption æ¨¡å‹ API æœåŠ¡å’Œå®¢æˆ·ç«¯å·¥å…·é›†ï¼Œæä¾›å›¾åƒæè¿°ç”Ÿæˆã€æ ‡ç­¾æå–ç­‰åŠŸèƒ½ã€‚

ğŸ“‹ ç›®å½•
åŠŸèƒ½ç‰¹æ€§
ç¯å¢ƒè¦æ±‚
å®‰è£…éƒ¨ç½²
ä½¿ç”¨æ•™ç¨‹
1. å¯åŠ¨æ¨¡å‹æœåŠ¡
2. å¯è§†åŒ–Webç•Œé¢
3. å‘½ä»¤è¡Œæ‰¹é‡å¤„ç†
4. ä»£ç è°ƒç”¨ç¤ºä¾‹
é…ç½®è¯´æ˜
æ³¨æ„äº‹é¡¹
ğŸ¯ åŠŸèƒ½ç‰¹æ€§
ğŸ–¼ï¸ æ™ºèƒ½å›¾åƒæè¿°ï¼šæ”¯æŒå¤šç§æè¿°é£æ ¼ï¼ˆè¯¦ç»†ã€éšæ„ã€ç›´æ¥ç­‰ï¼‰
ğŸ·ï¸ å›¾åƒæ ‡ç­¾ç”Ÿæˆï¼šç”Ÿæˆ Booru é£æ ¼æ ‡ç­¾
ğŸ­ æ··åˆæ¨¡å¼å¤„ç†ï¼šæ”¯æŒå¤šä¸ªæç¤ºè¯æ¨¡æ¿éšæœºç»„åˆ
ğŸ“ æ‰¹é‡å¤„ç†ï¼šæ”¯æŒå¤§æ‰¹é‡å›¾åƒæ–‡ä»¶å¤„ç†
ğŸŒ Webç•Œé¢ï¼šå‹å¥½çš„å¯è§†åŒ–æ“ä½œç•Œé¢
ğŸ”§ APIè°ƒç”¨ï¼šåŸºäº OpenAI å…¼å®¹çš„ API æ¥å£
âš¡ é«˜æ€§èƒ½ï¼šåŸºäº VLLM å¼•æ“ï¼Œæ”¯æŒ GPU åŠ é€Ÿ
ğŸ’» ç¯å¢ƒè¦æ±‚
ç³»ç»Ÿï¼šLinux / Windows / macOS
Pythonï¼š>= 3.8
GPUï¼šNVIDIA GPUï¼ˆæ¨èæ˜¾å­˜ >= 8GBï¼‰
CUDAï¼š>= 11.8
ä¾èµ–ï¼š
vllm
gradio
openai
PIL/Pillow
requests
ğŸš€ å®‰è£…éƒ¨ç½²
1. å…‹éš†é¡¹ç›®
git clone <your-repo-url>
cd JoyCaptionVLLM
2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ–è€…
venv\Scripts\activate     # Windows
3. å®‰è£…ä¾èµ–
pip install vllm gradio openai pillow requests
4. ä¸‹è½½æ¨¡å‹
# ä½¿ç”¨ Hugging Face Hub
pip install huggingface_hub
huggingface-cli download fancyfeast/llama-joycaption-alpha-two-hf-llava --local-dir ./llama-joycaption-alpha-two-hf-llava

# æˆ–è€…ä½¿ç”¨ git lfs
git lfs install
git clone https://huggingface.co/fancyfeast/llama-joycaption-alpha-two-hf-llava ./llama-joycaption-alpha-two-hf-llava
ğŸ“– ä½¿ç”¨æ•™ç¨‹
1. å¯åŠ¨æ¨¡å‹æœåŠ¡
ä½¿ç”¨æä¾›çš„å¯åŠ¨è„šæœ¬ï¼š

chmod +x start_command.sh
./start_command.sh
æˆ–è€…æ‰‹åŠ¨å¯åŠ¨ï¼š

CUDA_VISIBLE_DEVICES=0 vllm serve llama-joycaption-alpha-two-hf-llava \
    --max-model-len 4096 \
    --enable-prefix-caching \
    --port 8000
å‚æ•°è¯´æ˜ï¼š

CUDA_VISIBLE_DEVICES=0ï¼šæŒ‡å®šä½¿ç”¨çš„ GPU ç¼–å·
--max-model-len 4096ï¼šæœ€å¤§åºåˆ—é•¿åº¦
--enable-prefix-cachingï¼šå¯ç”¨å‰ç¼€ç¼“å­˜ï¼ˆæå‡æ€§èƒ½ï¼‰
--port 8000ï¼šAPI æœåŠ¡ç«¯å£
æœåŠ¡å¯åŠ¨æˆåŠŸåï¼ŒAPI å°†åœ¨ http://localhost:8000 å¯ç”¨ã€‚

2. å¯è§†åŒ–Webç•Œé¢
å¯åŠ¨ Gradio Web ç•Œé¢ï¼š

python app_mix.py --port 8888 --host 0.0.0.0
åŠŸèƒ½æ¨¡å—ï¼š

ğŸ”§ API é…ç½®
APIåœ°å€ï¼šè¾“å…¥æ¨¡å‹æœåŠ¡åœ°å€ï¼ˆå¦‚ï¼šhttp://192.168.5.212:8000/v1ï¼‰
APIå¯†é’¥ï¼šè¾“å…¥ API å¯†é’¥ï¼ˆå¯ä½¿ç”¨ä»»æ„å­—ç¬¦ä¸²ï¼‰
æµ‹è¯•è¿æ¥ï¼šç‚¹å‡»æµ‹è¯•æŒ‰é’®éªŒè¯è¿æ¥
ğŸ–¼ï¸ å•å›¾å¤„ç†
ä¸Šä¼ å›¾ç‰‡æ–‡ä»¶
é€‰æ‹©æè¿°ç±»å‹ï¼š
è¯¦ç»†æè¿°ï¼šæ­£å¼ã€è¯¦ç»†çš„æ•£æ–‡æè¿°
éšæ„æè¿°ï¼šå‹å¥½ã€å¯¹è¯å¼çš„æè¿°é£æ ¼
ç›´æ¥æè¿°ï¼šå®¢è§‚ã€ç®€æ´çš„è¦ç‚¹æè¿°
Stable Diffusionæç¤ºè¯ï¼šç”Ÿæˆ SD é£æ ¼çš„æç¤ºè¯
MidJourneyæç¤ºè¯ï¼šç”Ÿæˆ MJ é£æ ¼çš„æç¤ºè¯
è‰ºæœ¯è¯„è®ºï¼šè‰ºæœ¯å²é£æ ¼çš„ä¸“ä¸šåˆ†æ
äº§å“æ–‡æ¡ˆï¼šè¥é”€é£æ ¼çš„äº§å“æè¿°
ç¤¾åª’æ–‡æ¡ˆï¼šé€‚åˆç¤¾äº¤åª’ä½“çš„å¸å¼•äººæ–‡æ¡ˆ
è®¾ç½®æè¿°é•¿åº¦
é…ç½®é«˜çº§é€‰é¡¹ï¼ˆå¯é€‰ï¼‰
ç‚¹å‡»"ç”Ÿæˆæè¿°"
ğŸ“ æ‰¹é‡å¤„ç†
ä¸Šä¼ å¤šå¼ å›¾ç‰‡
é…ç½®ç›¸åŒçš„å¤„ç†å‚æ•°
ç‚¹å‡»"å¼€å§‹æ‰¹é‡å¤„ç†"
ä¸‹è½½åŒ…å«æ‰€æœ‰ç»“æœçš„ ZIP æ–‡ä»¶
ğŸ­ æ··åˆæ¨¡å¼æ‰¹é‡å¤„ç†
ä¸Šä¼ å¤šå¼ å›¾ç‰‡
é…ç½®å¤šä¸ªä¸åŒçš„æç¤ºè¯æ¨¡æ¿
ä¸ºæ¯ä¸ªæç¤ºè¯è®¾ç½®æƒé‡
ç³»ç»Ÿæ ¹æ®æƒé‡éšæœºåˆ†é…æç¤ºè¯
æŸ¥çœ‹å¤„ç†ç»Ÿè®¡å’Œä¸‹è½½ç»“æœ
3. å‘½ä»¤è¡Œæ‰¹é‡å¤„ç†
ä½¿ç”¨ image_captioning.py è¿›è¡Œæ‰¹é‡å¤„ç†ï¼š

åŸºç¡€ç”¨æ³•
# ç”Ÿæˆæ ‡ç­¾æ¨¡å¼
python image_captioning.py --input "/path/to/images" --mode tag

# ç”Ÿæˆæè¿°æ¨¡å¼
python image_captioning.py --input "/path/to/images" --mode des

# è‡ªå®šä¹‰æç¤ºè¯æ¨¡å¼
python image_captioning.py \
    --input "/path/to/images" \
    --mode custom \
    --custom_prompt "Write a detailed analysis of this image."
å®Œæ•´å‚æ•°
python image_captioning.py \
    --input "/path/to/images" \          # è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå¿…éœ€ï¼‰
    --output "/path/to/output" \         # è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºè¾“å…¥è·¯å¾„ï¼‰
    --api_key "your-api-key" \           # API å¯†é’¥ï¼ˆå¯é€‰ï¼‰
    --base_url "http://ip:8000/v1" \     # API åŸºç¡€åœ°å€ï¼ˆå¯é€‰ï¼‰
    --mode tag \                         # å¤„ç†æ¨¡å¼ï¼ˆå¿…éœ€ï¼‰
    --custom_prompt "prompt" \           # è‡ªå®šä¹‰æç¤ºè¯ï¼ˆä»… custom æ¨¡å¼ï¼‰
    --max_retries 3                      # æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆå¯é€‰ï¼‰
æ”¯æŒçš„å›¾åƒæ ¼å¼ï¼š.png, .jpg, .jpeg, .bmp, .gif, .webp

4. ä»£ç è°ƒç”¨ç¤ºä¾‹
å‚è€ƒ openai_client.py çš„å®ç°ï¼š

from openai import OpenAI
import base64

# åˆ›å»ºå®¢æˆ·ç«¯
client = OpenAI(
    api_key='your-api-key',
    base_url='http://192.168.5.212:8000/v1'
)

# å¤„ç†æœ¬åœ°å›¾ç‰‡
def process_local_image(image_path, prompt):
    with open(image_path, "rb") as image_file:
        base64_image = base64.b64encode(image_file.read()).decode("utf-8")
    
    image_data = f"data:image/jpeg;base64,{base64_image}"
    
    model_name = client.models.list().data[0].id
    response = client.chat.completions.create(
        model=model_name,
        messages=[
            {
                'role': 'system',
                'content': 'You are a helpful image captioner.',
            },
            {
                'role': 'user',
                'content': [
                    {'type': 'text', 'text': prompt},
                    {'type': 'image_url', 'image_url': {'url': image_data}}
                ],
            }
        ],
        temperature=0.9,
        top_p=0.7,
        max_tokens=256
    )
    
    return response.choices[0].message.content.strip()

# å¤„ç†ç½‘ç»œå›¾ç‰‡
def process_url_image(image_url, prompt):
    model_name = client.models.list().data[0].id
    response = client.chat.completions.create(
        model=model_name,
        messages=[
            {
                'role': 'system',
                'content': 'You are a helpful image captioner.',
            },
            {
                'role': 'user',
                'content': [
                    {'type': 'text', 'text': prompt},
                    {'type': 'image_url', 'image_url': {'url': image_url}}
                ],
            }
        ],
        temperature=0.9,
        top_p=0.7,
        max_tokens=256
    )
    
    return response.choices[0].message.content.strip()

# ä½¿ç”¨ç¤ºä¾‹
prompt = "Write a descriptive caption for this image in a casual tone."
result = process_local_image("/path/to/image.jpg", prompt)
print(result)
âš™ï¸ é…ç½®è¯´æ˜
æ¨¡å‹æœåŠ¡é…ç½®
ç¼–è¾‘ start_command.sh è‡ªå®šä¹‰å¯åŠ¨å‚æ•°ï¼š

#!/bin/bash
CUDA_VISIBLE_DEVICES=0 vllm serve llama-joycaption-alpha-two-hf-llava \
    --max-model-len 4096 \           # æœ€å¤§åºåˆ—é•¿åº¦
    --enable-prefix-caching \        # å¯ç”¨å‰ç¼€ç¼“å­˜
    --port 8000 \                    # API ç«¯å£
    --host 0.0.0.0 \                # ç›‘å¬åœ°å€
    --tensor-parallel-size 1 \       # å¼ é‡å¹¶è¡Œå¤§å°ï¼ˆå¤šGPUæ—¶ä½¿ç”¨ï¼‰
    --gpu-memory-utilization 0.9     # GPU å†…å­˜ä½¿ç”¨ç‡
Web ç•Œé¢é…ç½®
åœ¨ app_mix.py ä¸­ä¿®æ”¹é»˜è®¤é…ç½®ï¼š

# é»˜è®¤ API é…ç½®
DEFAULT_API_URL = "http://192.168.5.212:8000/v1"
DEFAULT_API_KEY = "your-api-key"

# å¯åŠ¨å‚æ•°
demo.launch(
    server_name="0.0.0.0",    # ç›‘å¬åœ°å€
    server_port=7860,         # Web ç•Œé¢ç«¯å£
    share=False,              # æ˜¯å¦åˆ›å»ºå…¬ç½‘é“¾æ¥
    show_error=True           # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
)
âš ï¸ æ³¨æ„äº‹é¡¹
GPU å†…å­˜ï¼šæ¨¡å‹éœ€è¦çº¦ 8GB æ˜¾å­˜ï¼Œç¡®ä¿ GPU èµ„æºå……è¶³
ç½‘ç»œé…ç½®ï¼šå¦‚éœ€ä»å…¶ä»–æœºå™¨è®¿é—®ï¼Œè¯·ç¡®ä¿é˜²ç«å¢™å…è®¸ç›¸åº”ç«¯å£
æ–‡ä»¶æƒé™ï¼šç¡®ä¿ç¨‹åºå¯¹è¾“å…¥/è¾“å‡ºæ–‡ä»¶å¤¹æœ‰è¯»å†™æƒé™
æ‰¹é‡å¤„ç†ï¼šå¤§æ‰¹é‡å¤„ç†æ—¶å»ºè®®åˆ†æ‰¹è¿›è¡Œï¼Œé¿å…å†…å­˜æº¢å‡º
API é™åˆ¶ï¼šæ³¨æ„ API çš„å¹¶å‘é™åˆ¶å’Œè¶…æ—¶è®¾ç½®
æ¨¡å‹æ›´æ–°ï¼šå®šæœŸæ£€æŸ¥æ¨¡å‹æ›´æ–°ï¼Œè·å–æœ€æ–°åŠŸèƒ½
ğŸ› æ•…éšœæ’æŸ¥
å¸¸è§é—®é¢˜
æ¨¡å‹åŠ è½½å¤±è´¥
   # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´
   ls -la ./models/llama-joycaption-alpha-two-hf-llava
   
   # é‡æ–°ä¸‹è½½æ¨¡å‹
   rm -rf ./models/llama-joycaption-alpha-two-hf-llava
   huggingface-cli download fancyfeast/llama-joycaption-alpha-two-hf-llava --local-dir ./models/llama-joycaption-alpha-two-hf-llava
   
GPU å†…å­˜ä¸è¶³
   # å‡å°‘æœ€å¤§åºåˆ—é•¿åº¦
   vllm serve llama-joycaption-alpha-two-hf-llava --max-model-len 2048
   
   # æˆ–è°ƒæ•´ GPU å†…å­˜ä½¿ç”¨ç‡
   vllm serve llama-joycaption-alpha-two-hf-llava --gpu-memory-utilization 0.8
   
API è¿æ¥å¤±è´¥

æ£€æŸ¥æ¨¡å‹æœåŠ¡æ˜¯å¦æ­£å¸¸å¯åŠ¨
ç¡®è®¤ IP åœ°å€å’Œç«¯å£é…ç½®æ­£ç¡®
éªŒè¯é˜²ç«å¢™è®¾ç½®
æ‰¹é‡å¤„ç†ZIPæ–‡ä»¶ä¸ºç©º

æ£€æŸ¥ app_mix.py ä¸­çš„æ–‡ä»¶å†™å…¥ä»£ç æ˜¯å¦è¢«æ³¨é‡Š
ç¡®ä¿è¾“å‡ºç›®å½•æœ‰å†™å…¥æƒé™
<a id="english">ğŸ¨ JoyCaption VLLM API Toolkit</a>
A comprehensive toolkit for JoyCaption model API service based on VLLM, providing image captioning, tag extraction, and other AI-powered image analysis features.

ğŸ“‹ Table of Contents
Features
System Requirements
Installation
Usage Guide
1. Start Model Service
2. Visual Web Interface
3. Command Line Batch Processing
4. Code Examples
Configuration
Troubleshooting
ğŸ¯ Features
ğŸ–¼ï¸ Intelligent Image Captioning: Multiple caption styles (detailed, casual, formal, etc.)
ğŸ·ï¸ Image Tag Generation: Booru-style tag generation
ğŸ­ Mixed Mode Processing: Random combination of multiple prompt templates
ğŸ“ Batch Processing: Support for large-scale image processing
ğŸŒ Web Interface: User-friendly visual interface
ğŸ”§ API Integration: OpenAI-compatible API interface
âš¡ High Performance: VLLM-powered with GPU acceleration
ğŸ’» System Requirements
OS: Linux / Windows / macOS
Python: >= 3.8
GPU: NVIDIA GPU (recommended VRAM >= 8GB)
CUDA: >= 11.8
Dependencies:
vllm
gradio
openai
PIL/Pillow
requests
ğŸš€ Installation
1. Clone Repository
git clone <your-repo-url>
cd JoyCaptionVLLM
2. Create Virtual Environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate     # Windows
3. Install Dependencies
pip install vllm gradio openai pillow requests
4. Download Model
# Using Hugging Face Hub
pip install huggingface_hub
huggingface-cli download fancyfeast/llama-joycaption-alpha-two-hf-llava --local-dir ./models/llama-joycaption-alpha-two-hf-llava

# Or using git lfs
git lfs install
git clone https://huggingface.co/fancyfeast/llama-joycaption-alpha-two-hf-llava ./models/llama-joycaption-alpha-two-hf-llava
ğŸ“– Usage Guide
1. Start Model Service
Using the provided startup script:

chmod +x start_command.sh
./start_command.sh
Or start manually:

CUDA_VISIBLE_DEVICES=0 vllm serve llama-joycaption-alpha-two-hf-llava \
    --max-model-len 4096 \
    --enable-prefix-caching \
    --port 8000
Parameter Explanation:

CUDA_VISIBLE_DEVICES=0: Specify GPU device number
--max-model-len 4096: Maximum sequence length
--enable-prefix-caching: Enable prefix caching for better performance
--port 8000: API service port
After successful startup, the API will be available at http://localhost:8000.

2. Visual Web Interface
Launch Gradio Web interface:

python app_mix.py --port 7860 --host 0.0.0.0
Interface Modules:

ğŸ”§ API Configuration
API URL: Enter model service address (e.g., http://192.168.5.212:8000/v1)
API Key: Enter API key (any string works)
Test Connection: Click test button to verify connection
ğŸ–¼ï¸ Single Image Processing
Upload image file
Select caption type:
Detailed Description: Formal, comprehensive prose description
Casual Description: Friendly, conversational style
Direct Description: Objective, concise point-based description
Stable Diffusion Prompts: Generate SD-style prompts
MidJourney Prompts: Generate MJ-style prompts
Art Critique: Art history style professional analysis
Product Copy: Marketing-style product description
Social Media Copy: Social media friendly engaging content
Set caption length
Configure advanced options (optional)
Click "Generate Caption"
ğŸ“ Batch Processing
Upload multiple images
Configure processing parameters
Click "Start Batch Processing"
Download ZIP file with all results
ğŸ­ Mixed Mode Batch Processing
Upload multiple images
Configure multiple prompt templates
Set weights for each prompt
System randomly assigns prompts based on weights
View processing statistics and download results
3. Command Line Batch Processing
Use image_captioning.py for batch processing:

Basic Usage
# Tag generation mode
python image_captioning.py --input "/path/to/images" --mode tag

# Description generation mode
python image_captioning.py --input "/path/to/images" --mode des

# Custom prompt mode
python image_captioning.py \
    --input "/path/to/images" \
    --mode custom \
    --custom_prompt "Write a detailed analysis of this image."
Full Parameters
python image_captioning.py \
    --input "/path/to/images" \          # Input folder path (required)
    --output "/path/to/output" \         # Output folder path (optional, defaults to input path)
    --api_key "your-api-key" \           # API key (optional)
    --base_url "http://ip:8000/v1" \     # API base URL (optional)
    --mode tag \                         # Processing mode (required)
    --custom_prompt "prompt" \           # Custom prompt (custom mode only)
    --max_retries 3                      # Maximum retry attempts (optional)
Supported Image Formats: .png, .jpg, .jpeg, .bmp, .gif, .webp

4. Code Examples
Reference implementation from openai_client.py:

from openai import OpenAI
import base64

# Create client
client = OpenAI(
    api_key='your-api-key',
    base_url='http://192.168.5.212:8000/v1'
)

# Process local image
def process_local_image(image_path, prompt):
    with open(image_path, "rb") as image_file:
        base64_image = base64.b64encode(image_file.read()).decode("utf-8")
    
    image_data = f"data:image/jpeg;base64,{base64_image}"
    
    model_name = client.models.list().data[0].id
    response = client.chat.completions.create(
        model=model_name,
        messages=[
            {
                'role': 'system',
                'content': 'You are a helpful image captioner.',
            },
            {
                'role': 'user',
                'content': [
                    {'type': 'text', 'text': prompt},
                    {'type': 'image_url', 'image_url': {'url': image_data}}
                ],
            }
        ],
        temperature=0.9,
        top_p=0.7,
        max_tokens=256
    )
    
    return response.choices[0].message.content.strip()

# Process image from URL
def process_url_image(image_url, prompt):
    model_name = client.models.list().data[0].id
    response = client.chat.completions.create(
        model=model_name,
        messages=[
            {
                'role': 'system',
                'content': 'You are a helpful image captioner.',
            },
            {
                'role': 'user',
                'content': [
                    {'type': 'text', 'text': prompt},
                    {'type': 'image_url', 'image_url': {'url': image_url}}
                ],
            }
        ],
        temperature=0.9,
        top_p=0.7,
        max_tokens=256
    )
    
    return response.choices[0].message.content.strip()

# Usage example
prompt = "Write a descriptive caption for this image in a casual tone."
result = process_local_image("/path/to/image.jpg", prompt)
print(result)
âš™ï¸ Configuration
Model Service Configuration
Edit start_command.sh to customize startup parameters:

#!/bin/bash
CUDA_VISIBLE_DEVICES=0 vllm serve llama-joycaption-alpha-two-hf-llava \
    --max-model-len 4096 \           # Maximum sequence length
    --enable-prefix-caching \        # Enable prefix caching
    --port 8000 \                    # API port
    --host 0.0.0.0 \                # Listen address
    --tensor-parallel-size 1 \       # Tensor parallel size (for multi-GPU)
    --gpu-memory-utilization 0.9     # GPU memory utilization
Web Interface Configuration
Modify default settings in app_mix.py:

# Default API configuration
DEFAULT_API_URL = "http://192.168.5.212:8000/v1"
DEFAULT_API_KEY = "your-api-key"

# Launch parameters
demo.launch(
    server_name="0.0.0.0",    # Listen address
    server_port=7860,         # Web interface port
    share=False,              # Create public link
    show_error=True           # Show error messages
)
ğŸ› Troubleshooting
Common Issues
Model Loading Failed
   # Check if model files are complete
   ls -la ./models/llama-joycaption-alpha-two-hf-llava
   
   # Re-download model
   rm -rf ./models/llama-joycaption-alpha-two-hf-llava
   huggingface-cli download fancyfeast/llama-joycaption-alpha-two-hf-llava --local-dir ./models/llama-joycaption-alpha-two-hf-llava
   
GPU Memory Insufficient
   # Reduce maximum sequence length
   vllm serve llama-joycaption-alpha-two-hf-llava --max-model-len 2048
   
   # Or adjust GPU memory utilization
   vllm serve llama-joycaption-alpha-two-hf-llava --gpu-memory-utilization 0.8
   
API Connection Failed

Check if model service is running properly
Verify IP address and port configuration
Check firewall settings
Empty ZIP File in Batch Processing

Check if file writing code in app_mix.py is commented out
Ensure output directory has write permissions
ğŸ“ License
This project is licensed under the MIT License - see the LICENSE file for details.

ğŸ¤ Contributing
Contributions are welcome! Please feel free to submit a Pull Request.

ğŸ“ Support
If you encounter any issues, please:

Check the troubleshooting section
Search existing issues
Create a new issue with detailed information
Keywords: JoyCaption, VLLM, Image Captioning, Computer Vision, AI, Machine Learning, Image Analysis, Batch Processing